{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71dfa49a",
   "metadata": {},
   "source": [
    "# LNovelD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9d0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.modules.lnoveld import LNovelD\n",
    "\n",
    "m = LNovelD(2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4419e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LNovelD(\n",
       "  (obs_noveld): NovelD(\n",
       "    (target): MLPNetwork(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (5): Linear(in_features=64, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (predictor): MLPNetwork(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (5): Linear(in_features=64, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lang_noveld): NovelD(\n",
       "    (target): MLPNetwork(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (5): Linear(in_features=64, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (predictor): MLPNetwork(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (5): Linear(in_features=64, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52519d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target tensor([[-1.8384,  0.9695, -3.8054, -2.1500,  3.6722]])\n",
      "pred tensor([[ 2.2023, -4.3319, -0.7381, -3.4516,  0.7183]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "nov tensor([8.0162])\n",
      "last nov tensor([0.8016])\n",
      "comp tensor([-3.2065])\n",
      "7.615405559539795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.615405559539795"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.Tensor([[10.0, 10.0]])\n",
    "m(obs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a8d4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1.0, 1.0): 1, (10.0, 10.0): 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.obs_noveld.episode_states_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e5793",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef5076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0602, -0.1766, -0.0372,  0.1486,  0.3524,  0.0492, -0.0064,\n",
       "           0.1933, -0.3469, -0.0596],\n",
       "         [ 0.1362, -0.3001,  0.0173,  0.2752,  0.3259, -0.0171,  0.0233,\n",
       "           0.0974, -0.2904, -0.0622],\n",
       "         [ 0.0602, -0.1766, -0.0372,  0.1486,  0.3524,  0.0492, -0.0064,\n",
       "           0.1933, -0.3469, -0.0596],\n",
       "         [ 0.1281, -0.2525,  0.0050,  0.2414,  0.3356,  0.0387,  0.0285,\n",
       "           0.1468, -0.3164, -0.0694],\n",
       "         [ 0.1491, -0.2382,  0.0952,  0.1408,  0.3274,  0.1251, -0.0039,\n",
       "           0.1219, -0.3603,  0.0718]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from model.modules.lm import GRUEncoder, OneHotEncoder\n",
    "\n",
    "gru = GRUEncoder(\n",
    "    10, \n",
    "    OneHotEncoder(['South','Not','Located','West','Object','Landmark','North','Center','East']))\n",
    "\n",
    "opt = torch.optim.Adam(gru.gru.parameters(), lr=0.1)\n",
    "\n",
    "sentences = [\n",
    "    [\"Located\", \"South\"], \n",
    "    [\"Located\", \"Center\", \"Object\", \"East\"], \n",
    "    [\"Located\", \"South\"], \n",
    "    [\"Located\", \"East\", \"Object\", \"South\", \"East\"], \n",
    "    [\"Located\", \"South\", \"West\"]\n",
    "]\n",
    "sentences = [[\"<SOS>\"] + s + [\"<EOS>\"] for s in sentences]\n",
    "\n",
    "encs = gru(sentences)\n",
    "gru(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93b6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "encs.mean().backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4038d2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4974, -0.3791, -0.1940, -0.3039, -0.1669, -0.2489, -0.5031,\n",
       "          -0.1984, -0.5563, -0.0602],\n",
       "         [-0.4415, -0.3436, -0.3280, -0.3862, -0.2037, -0.2943, -0.6240,\n",
       "          -0.2357, -0.5833,  0.0067],\n",
       "         [-0.4974, -0.3791, -0.1940, -0.3039, -0.1669, -0.2489, -0.5031,\n",
       "          -0.1984, -0.5563, -0.0602],\n",
       "         [-0.4591, -0.3452, -0.3098, -0.3782, -0.2319, -0.2676, -0.6360,\n",
       "          -0.1869, -0.5940,  0.0045],\n",
       "         [-0.5177, -0.3911, -0.2654, -0.3371, -0.1657, -0.3111, -0.5693,\n",
       "          -0.2637, -0.5415, -0.0435]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad344c7f",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5dfb1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.3735, -2.4502, -2.6180, -2.0134, -2.3471, -2.8143, -2.6677,\n",
       "           -1.8240, -2.4765, -2.5626, -2.7094]]], grad_fn=<LogSoftmaxBackward0>),\n",
       " tensor([[[ 0.6252,  0.7289,  0.8356,  0.5018,  0.7564,  0.8033,  0.7864,\n",
       "            0.7227, -0.0403,  0.5622]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from model.modules.lm import GRUDecoder, OneHotEncoder\n",
    "\n",
    "word_encoder = OneHotEncoder(\n",
    "    ['South','Not','Located','West','Object','Landmark','North','Center','East'])\n",
    "dec = GRUDecoder(10, word_encoder)\n",
    "\n",
    "last_hidden = torch.ones((1, 1, 10))\n",
    "last_word = torch.Tensor(word_encoder.SOS_ENC).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "dec.forward_step(last_word, last_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e744b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "Token # 0\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Token # 1\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.6252,  0.7289,  0.8356,  0.5018,  0.7564,  0.8033,  0.7864,\n",
      "           0.7227, -0.0403,  0.5622]]], grad_fn=<StackBackward0>)\n",
      "Token # 2\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.4122,  0.6449,  0.6308,  0.2169,  0.6463,  0.6911,  0.6096,\n",
      "           0.3685, -0.2815,  0.2256]]], grad_fn=<StackBackward0>)\n",
      "Token # 3\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.1948,  0.4077,  0.4849,  0.1084,  0.5570,  0.3894,  0.4387,\n",
      "           0.1903, -0.1325, -0.0323]]], grad_fn=<StackBackward0>)\n",
      "Batch # 1\n",
      "Token # 0\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Token # 1\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.6252,  0.7289,  0.8356,  0.5018,  0.7564,  0.8033,  0.7864,\n",
      "           0.7227, -0.0403,  0.5622]]], grad_fn=<StackBackward0>)\n",
      "Token # 2\n",
      "torch.Size([1, 1, 11])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.4122,  0.6449,  0.6308,  0.2169,  0.6463,  0.6911,  0.6096,\n",
      "           0.3685, -0.2815,  0.2256]]], grad_fn=<StackBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([[[[-2.3735, -2.4502, -2.6180, -2.0134, -2.3471, -2.8143, -2.6677,\n",
       "             -1.8240, -2.4765, -2.5626, -2.7094]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.3814, -2.4873, -2.5747, -2.1063, -2.3153, -2.6529, -2.6838,\n",
       "             -1.9245, -2.4462, -2.4562, -2.6485]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.4006, -2.5280, -2.5888, -2.1507, -2.3011, -2.6244, -2.6682,\n",
       "             -2.0683, -2.2993, -2.4048, -2.5383]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.4772, -2.5148, -2.5760, -2.1886, -2.3192, -2.5286, -2.5830,\n",
       "             -2.1398, -2.2926, -2.3642, -2.5180]]]], grad_fn=<StackBackward0>),\n",
       "  tensor([[[[-2.3735, -2.4502, -2.6180, -2.0134, -2.3471, -2.8143, -2.6677,\n",
       "             -1.8240, -2.4765, -2.5626, -2.7094]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.3814, -2.4873, -2.5747, -2.1063, -2.3153, -2.6529, -2.6838,\n",
       "             -1.9245, -2.4462, -2.4562, -2.6485]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.4689, -2.4843, -2.5507, -2.1595, -2.3206, -2.5464, -2.5999,\n",
       "             -2.0445, -2.3717, -2.4071, -2.5927]]]], grad_fn=<StackBackward0>)],\n",
       " [[], []])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.ones(2, 10)\n",
    "targets = [\n",
    "    torch.Tensor([\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
    "    torch.Tensor([\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "]\n",
    "\n",
    "dec(context, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7db7d9",
   "metadata": {},
   "source": [
    "# Train Encoder-Decoder\n",
    "\n",
    "Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08cb10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_sentences(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    sentences = []\n",
    "    for step, s_data in data.items():\n",
    "        if not step.startswith(\"Step\"):\n",
    "            continue\n",
    "        sentences.append(s_data[\"Agent_0\"][\"Sentence\"][1:-1])\n",
    "        sentences.append(s_data[\"Agent_1\"][\"Sentence\"][1:-1])\n",
    "    return sentences\n",
    "sentences = load_sentences(\"test_data/Sentences_Generated_P1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d23934",
   "metadata": {},
   "source": [
    "Initialise Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c58c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.modules.lm import GRUEncoder, GRUDecoder, OneHotEncoder\n",
    "\n",
    "word_encoder = OneHotEncoder(\n",
    "    ['South','Not','Located','West','Object','Landmark','North','Center','East'])\n",
    "\n",
    "enc = GRUEncoder(32, word_encoder)\n",
    "dec = GRUDecoder(32, word_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc9e0e",
   "metadata": {},
   "source": [
    "Initialise Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952b919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optim = optim.SGD(list(enc.parameters()) + list(dec.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db73a85",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d57048",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 20000/20000 [45:00<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "def train(sentences, enc, dec, word_encoder, criterion, optim, n_iters=20000, batch_size=64):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    \n",
    "    for s_i in tqdm(range(n_iters)):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(sentences, batch_size)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        enc_hidden_state = enc(batch)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        enc_hidden_state.squeeze(0)\n",
    "        encoded_targets = word_encoder.encode_batch(batch)\n",
    "        decoder_outputs, _ = dec(enc_hidden_state.squeeze(0), encoded_targets)\n",
    "        \n",
    "        # Compute loss\n",
    "        tot_len = 0\n",
    "        for d_o, e_t in zip(decoder_outputs, encoded_targets):\n",
    "            e_t = torch.argmax(e_t, dim=1)\n",
    "            loss += criterion(d_o, e_t)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        plot_losses.append(loss.item())\n",
    "    \n",
    "    return plot_losses\n",
    "            \n",
    "plot_loss = train(sentences, enc, dec, word_encoder, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc0e23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65a60a8e50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20a56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss = [l / 64 for l in plot_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c3da5",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd0c00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sentences[8]\n",
    "s_enc = enc([s])\n",
    "_, s_pred = dec(s_enc)\n",
    "s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2305ce46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Located', 'East', 'Landmark', 'West']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c749f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Located', 'East', 'Landmark', 'West']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_encoder.decode_batch(s_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d220d4",
   "metadata": {},
   "source": [
    "# Observation captioning\n",
    "\n",
    "Let's try to learn to generate the sentence corresponding to a given observation.\n",
    "\n",
    "The Observation Encoder will be trained to encode the observation, and the Decoder will have to predict the sentence description of the observation.\n",
    "\n",
    "*First step:* Load observation-sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b103992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': [0.3566717617981794,\n",
       "  0.22018956073987805,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -0.4988445287954038,\n",
       "  -0.25249793046375035],\n",
       " 'sentence': ['Located', 'East', 'Landmark', 'South', 'West']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_pairs(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    pairs = []\n",
    "    for step, s_data in data.items():\n",
    "        if not step.startswith(\"Step\"):\n",
    "            continue\n",
    "        pairs.append({\n",
    "            \"observation\": s_data[\"Agent_0\"][\"Observation\"],\n",
    "            \"sentence\": s_data[\"Agent_0\"][\"Sentence\"][1:-1]\n",
    "        })\n",
    "        pairs.append({\n",
    "            \"observation\": s_data[\"Agent_1\"][\"Observation\"],\n",
    "            \"sentence\": s_data[\"Agent_1\"][\"Sentence\"][1:-1]\n",
    "        })\n",
    "    return pairs\n",
    "\n",
    "data_pairs = load_pairs(\"test_data/Sentences_Generated_P1.json\")\n",
    "data_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c209e2",
   "metadata": {},
   "source": [
    "Initialise the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0479e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.modules.lm import GRUDecoder, OneHotEncoder\n",
    "from model.modules.obs import ObservationEncoder\n",
    "\n",
    "word_encoder = OneHotEncoder(\n",
    "    ['South','Not','Located','West','Object','Landmark','North','Center','East'])\n",
    "\n",
    "dec = GRUDecoder(32, word_encoder)\n",
    "\n",
    "obs_enc = ObservationEncoder(17, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246b2c5",
   "metadata": {},
   "source": [
    "Initialise loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e726a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optim = optim.SGD(list(dec.parameters()) + list(obs_enc.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d751954",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████                               | 5092/10000 [10:47<09:41,  8.43it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "def train(data, obs_enc, dec, word_encoder, criterion, optim, n_iters=10000, batch_size=64):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    \n",
    "    for s_i in tqdm(range(n_iters)):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(data, batch_size)\n",
    "        obs_batch = []\n",
    "        sent_batch = []\n",
    "        for pair in batch:\n",
    "            obs_batch.append(pair[\"observation\"])\n",
    "            sent_batch.append(pair[\"sentence\"])\n",
    "        \n",
    "        # Encode observations\n",
    "        obs_tensor = torch.Tensor(np.array(obs_batch))\n",
    "        context_batch = obs_enc(obs_tensor)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        encoded_targets = word_encoder.encode_batch(sent_batch)\n",
    "        decoder_outputs, _ = dec(context_batch, encoded_targets)\n",
    "        \n",
    "        # Compute loss\n",
    "        tot_len = 0\n",
    "        for d_o, e_t in zip(decoder_outputs, encoded_targets):\n",
    "            e_t = torch.argmax(e_t, dim=1)\n",
    "            loss += criterion(d_o, e_t)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        plot_losses.append(loss.item() / batch_size)\n",
    "    \n",
    "    return plot_losses\n",
    "            \n",
    "plot_loss = train(data_pairs, obs_enc, dec, word_encoder, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965eec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8895d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
