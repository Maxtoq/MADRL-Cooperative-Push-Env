{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7949bb",
   "metadata": {},
   "source": [
    "## Add Embedding layers to Language Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from model.modules.lm import OneHotEncoder, init_rnn_params\n",
    "from model.modules.obs import ObservationEncoder\n",
    "\n",
    "\n",
    "class GRUEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for a language encoder using a Gated Recurrent Unit network\n",
    "    \"\"\"\n",
    "    def __init__(self, context_dim, hidden_dim, word_encoder, \n",
    "                 n_layers=1, device='cpu'):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            :param context_dim (int): Dimension of the context vectors (output\n",
    "                of the model).\n",
    "            :param hidden_dim (int): Dimension of the hidden state of the GRU\n",
    "                newtork.\n",
    "            :param word_encoder (OneHotEncoder): Word encoder, associating \n",
    "                tokens with one-hot encodings\n",
    "            :param n_layers (int): number of layers in the GRU (default: 1)\n",
    "            :param device (str): CUDA device\n",
    "        \"\"\"\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_encoder = word_encoder\n",
    "        self.context_dim = context_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(self.word_encoder.enc_dim, )\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.word_encoder.enc_dim, \n",
    "            self.context_dim, \n",
    "            n_layers,\n",
    "            batch_first=True)\n",
    "        init_rnn_params(self.gru)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, context_dim)\n",
    "        self.norm = nn.LayerNorm(context_dim)\n",
    "\n",
    "    def forward(self, sent_batch):\n",
    "        \"\"\"\n",
    "        Transforms sentences into embeddings\n",
    "        Inputs:\n",
    "            :param sentence_batch (list(list(str))): Batch of sentences.\n",
    "        Outputs:\n",
    "            :param unsorted_hstates (torch.Tensor): Final hidden states\n",
    "                corresponding to each given sentence, dim=(1, batch_size, \n",
    "                context_dim)\n",
    "        \"\"\"\n",
    "        # Get one-hot encodings\n",
    "        enc_sent_batch = self.word_encoder.encode_batch(sent_batch)\n",
    "\n",
    "        # Get order of sententes sorted by length decreasing\n",
    "        ids = sorted(\n",
    "            range(len(enc_sent_batch)), \n",
    "            key=lambda x: len(enc_sent_batch[x]), \n",
    "            reverse=True)\n",
    "\n",
    "        # Sort the sentences by length\n",
    "        sorted_list = [enc_sent_batch[i] for i in ids]\n",
    "\n",
    "        # Pad sentences\n",
    "        padded = nn.utils.rnn.pad_sequence(\n",
    "            sorted_list, batch_first=True)\n",
    "\n",
    "        # Pack padded sentences (to not care about padded tokens)\n",
    "        lens = [len(s) for s in sorted_list]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            padded, lens, batch_first=True).to(self.device)\n",
    "\n",
    "        # Initial hidden state\n",
    "        hidden = torch.zeros(1, len(enc_sent_batch), self.context_dim, \n",
    "                        device=self.device)\n",
    "\n",
    "        # Pass sentences into GRU model\n",
    "        _, hidden_states = self.gru(packed, hidden)\n",
    "\n",
    "        # Re-order hidden states\n",
    "        unsorted_hstates = torch.zeros_like(hidden_states).to(self.device)\n",
    "        unsorted_hstates[0,ids,:] = hidden_states[0,:,:]\n",
    "\n",
    "        return self.norm(unsorted_hstates)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {'gru': self.gru.state_dict(),\n",
    "                'out': self.out.state_dict()}\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for a language decoder using a Gated Recurrent Unit network\n",
    "    \"\"\"\n",
    "    def __init__(self, context_dim, word_encoder, n_layers=1, max_length=15,\n",
    "                 device='cpu'):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            :param context_dim (int): Dimension of the context vectors\n",
    "            :param word_encoder (OneHotEncoder): Word encoder, associating \n",
    "                tokens with one-hot encodings\n",
    "            :param n_layers (int): number of layers in the GRU (default: 1)\n",
    "            :param device (str): CUDA device\n",
    "        \"\"\"\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.device = device\n",
    "        # Dimension of hidden states\n",
    "        self.hidden_dim = context_dim\n",
    "        # Word encoder\n",
    "        self.word_encoder = word_encoder\n",
    "        # Number of recurrent layers\n",
    "        self.n_layers = n_layers\n",
    "        # Max length of generated sentences\n",
    "        self.max_length = max_length\n",
    "        # Model\n",
    "        self.gru = nn.GRU(\n",
    "            self.word_encoder.enc_dim, \n",
    "            self.hidden_dim, \n",
    "            self.n_layers)\n",
    "        init_rnn_params(self.gru)\n",
    "        # Output layer\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.word_encoder.enc_dim),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "\n",
    "    def forward_step(self, last_token, last_hidden):\n",
    "        \"\"\"\n",
    "        Generate prediction from GRU network.\n",
    "        Inputs:\n",
    "            :param last_token (torch.Tensor): Token at last time step, \n",
    "                dim=(1, 1, token_dim).\n",
    "            :param last_hidden (torch.Tensor): Hidden state of the GRU at last\n",
    "                time step, dim=(1, 1, hidden_dim).\n",
    "        Outputs:\n",
    "            :param output (torch.Tensor): Log-probabilities outputed by the \n",
    "                model, dim=(1, 1, token_dim).\n",
    "            :param hidden (torch.Tensor): New hidden state of the GRU network,\n",
    "                dim=(1, 1, hidden_dim).\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(last_token, last_hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def forward(self, context_batch, target_encs=None):\n",
    "        \"\"\"\n",
    "        Transforms context vectors to sentences\n",
    "        Inputs:\n",
    "            :param context_batch (torch.Tensor): Batch of context vectors,\n",
    "                dim=(batch_size, context_dim).\n",
    "            :param target_encs (list): Batch of target encoded sentences used\n",
    "                for teacher forcing. If None then no teacher forcing. \n",
    "                (Default: None)\n",
    "        Outputs:\n",
    "            :param decoder_outputs (list): Batch of tensors containing\n",
    "                log-probabilities generated by the GRU network.\n",
    "            :param sentences (list): Sentences generated with greedy \n",
    "                sampling. Empty if target_encs is not None (teacher forcing,\n",
    "                so we only care about model predictions).\n",
    "        \"\"\"\n",
    "        teacher_forcing = target_encs is not None\n",
    "        batch_size = context_batch.size(0)\n",
    "        max_sent_len = max([len(s) for s in target_encs]) if teacher_forcing \\\n",
    "            else self.max_length\n",
    "\n",
    "        hidden = context_batch.unsqueeze(0)\n",
    "        last_tokens = torch.Tensor(self.word_encoder.SOS_ENC).view(\n",
    "            1, 1, -1).float().repeat(1, batch_size, 1).to(self.device)\n",
    "\n",
    "        tokens = []\n",
    "        decoder_outputs = []\n",
    "        sentences = [[] for b_i in range(batch_size)]\n",
    "        sent_finished = [False] * batch_size\n",
    "        for t_i in range(max_sent_len):\n",
    "            # RNN pass\n",
    "            outputs, hidden = self.forward_step(last_tokens, hidden)\n",
    "            decoder_outputs.append(outputs)\n",
    "\n",
    "            # Sample next tokens\n",
    "            if teacher_forcing:\n",
    "                last_tokens = torch.zeros_like(last_tokens).to(self.device)\n",
    "                for b_i in range(batch_size):\n",
    "                    if t_i < target_encs[b_i].size(0):\n",
    "                        last_tokens[0, b_i] = target_encs[b_i][t_i]\n",
    "            else:\n",
    "                _, topi = outputs.topk(1)\n",
    "                topi = topi.squeeze()\n",
    "                last_tokens = torch.Tensor(\n",
    "                    self.word_encoder.token_encodings[topi.cpu()]).unsqueeze(0).to(\n",
    "                        self.device)\n",
    "\n",
    "                for b_i in range(batch_size):\n",
    "                    if topi[b_i] == self.word_encoder.EOS_ID:\n",
    "                        sent_finished[b_i] = True\n",
    "                    if not sent_finished[b_i]:\n",
    "                        sentences[b_i].append(\n",
    "                            self.word_encoder.index2token(topi[b_i]))\n",
    "                \n",
    "                if all(sent_finished):\n",
    "                    break\n",
    "                    \n",
    "        decoder_outputs = torch.cat(decoder_outputs, axis=0).transpose(0, 1)\n",
    "\n",
    "        return decoder_outputs, sentences\n",
    "\n",
    "    def compute_pp(self, enc_sent):\n",
    "        \"\"\"\n",
    "        :param enc_sent: (list(torch.Tensor))\n",
    "        \"\"\"\n",
    "        batch_size = len(enc_sent)\n",
    "        max_sent_len = max([len(s) for s in enc_sent])\n",
    "\n",
    "        hidden = torch.zeros((self.n_layers, batch_size, self.hidden_dim))\n",
    "        last_tokens = torch.Tensor(self.word_encoder.SOS_ENC).view(\n",
    "            1, 1, -1).repeat(1, batch_size, 1).to(self.device)\n",
    "\n",
    "        pnorm = torch.ones(batch_size)\n",
    "        for t_i in range(max_sent_len):\n",
    "            # RNN pass\n",
    "            outputs, hidden = self.forward_step(last_tokens, hidden)\n",
    "\n",
    "            # Compute PP\n",
    "            probs = outputs.exp().squeeze(0)\n",
    "            for s_i in range(batch_size):\n",
    "                len_s = enc_sent[s_i].size(0)\n",
    "                if t_i < len_s:\n",
    "                    token_prob = (probs[s_i] * enc_sent[s_i][t_i]).sum(-1)\n",
    "                    pnorm[s_i] *= (token_prob ** (1 / len_s))\n",
    "\n",
    "            # Do teacher forcing\n",
    "            last_tokens = torch.zeros_like(last_tokens).to(self.device)\n",
    "            for s_i in range(batch_size):\n",
    "                if t_i < enc_sent[s_i].size(0):\n",
    "                    last_tokens[0, s_i] = enc_sent[s_i][t_i]\n",
    "\n",
    "        pp = 1 / pnorm\n",
    "\n",
    "        return pp\n",
    "\n",
    "    def get_params(self):\n",
    "        return {'gru': self.gru.state_dict(),\n",
    "                'out': self.out.state_dict()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3d5f9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b774bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def init_training_objects(voc, context_dim, obs_dim, lr):\n",
    "    word_encoder = OneHotEncoder(voc)\n",
    "\n",
    "    lang_enc = GRUEncoder(context_dim, word_encoder)\n",
    "    dec = GRUDecoder(context_dim, word_encoder)\n",
    "\n",
    "    obs_enc = ObservationEncoder(obs_dim, context_dim)\n",
    "\n",
    "    cross_ent_l = nn.CrossEntropyLoss()\n",
    "    nll_l = nn.NLLLoss()\n",
    "    opt = optim.Adam(list(lang_enc.parameters()) + list(dec.parameters()) + list(obs_enc.parameters()), lr=lr)\n",
    "    \n",
    "    return word_encoder, lang_enc, obs_enc, dec, cross_ent_l, nll_l, opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
