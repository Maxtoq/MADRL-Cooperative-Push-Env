{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from src.algo.language.lm import OneHotEncoder, GRUEncoder, init_rnn_params\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for a language decoder using a Gated Recurrent Unit network\n",
    "    \"\"\"\n",
    "    def __init__(self, context_dim, embed_dim, word_encoder, max_len, \n",
    "                 n_layers=1, embed_layer= None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            :param context_dim (int): Dimension of the context vectors\n",
    "            :param word_encoder (OneHotEncoder): Word encoder, associating \n",
    "                tokens with one-hot encodings\n",
    "            :param n_layers (int): number of layers in the GRU (default: 1)\n",
    "            :param device (str): CUDA device\n",
    "        \"\"\"\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        # Dimension of hidden states\n",
    "        self.hidden_dim = context_dim\n",
    "        # Word encoder\n",
    "        self.word_encoder = word_encoder\n",
    "        # Number of recurrent layers\n",
    "        self.n_layers = n_layers\n",
    "        # Embedding layer\n",
    "        if embed_layer is not None:\n",
    "            self.embed_layer = embed_layer\n",
    "        else:\n",
    "            self.embed_layer = nn.Embedding(self.word_encoder.enc_dim, embed_dim)\n",
    "        # Model\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.n_layers)\n",
    "        init_rnn_params(self.gru)\n",
    "        # Output layer\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.word_encoder.enc_dim),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "\n",
    "    def forward_step(self, last_token, last_hidden):\n",
    "        \"\"\"\n",
    "        Generate prediction from GRU network.\n",
    "        Inputs:\n",
    "            :param last_token (torch.Tensor): Token at last time step, \n",
    "                dim=(1, 1, token_dim).\n",
    "            :param last_hidden (torch.Tensor): Hidden state of the GRU at last\n",
    "                time step, dim=(1, 1, hidden_dim).\n",
    "        Outputs:\n",
    "            :param output (torch.Tensor): Log-probabilities outputed by the \n",
    "                model, dim=(1, 1, token_dim).\n",
    "            :param hidden (torch.Tensor): New hidden state of the GRU network,\n",
    "                dim=(1, 1, hidden_dim).\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(last_token, last_hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def forward(self, context_batch, target_encs=None):\n",
    "        \"\"\"\n",
    "        Transforms context vectors to sentences\n",
    "        Inputs:\n",
    "            :param context_batch (torch.Tensor): Batch of context vectors,\n",
    "                dim=(batch_size, context_dim).\n",
    "            :param target_encs (torch.Tensor): Batch of target sentences used\n",
    "                for teacher forcing, encoded as onehots and padded with -1, \n",
    "                dim=(batch_size, max_sent_len, enc_dim). If None then no \n",
    "                teacher forcing. Default: None.\n",
    "        Outputs:\n",
    "            :param decoder_outputs (list): Batch of tensors containing\n",
    "                log-probabilities generated by the GRU network.\n",
    "            :param sentences (list): Sentences generated with greedy \n",
    "                sampling. Empty if target_encs is not None (teacher forcing,\n",
    "                so we only care about model predictions).\n",
    "        \"\"\"\n",
    "        teacher_forcing = target_encs is not None\n",
    "        batch_size = context_batch.size(0)\n",
    "        max_sent_len = target_encs.shape[1] if teacher_forcing \\\n",
    "            else self.max_len\n",
    "\n",
    "        if teacher_forcing:\n",
    "            # Embed\n",
    "            target_ids = target_encs.argmax(-1)\n",
    "            target_embeds = self.embed_layer(target_ids)\n",
    "\n",
    "        hidden = context_batch.unsqueeze(0)\n",
    "        # Init last token to the SOS token, embedded\n",
    "        last_tokens = self.embed_layer(\n",
    "            torch.zeros((1, batch_size), dtype=torch.int).to(self.device))\n",
    "\n",
    "        tokens = []\n",
    "        decoder_outputs = []\n",
    "        # sentences = [[] for b_i in range(batch_size)]\n",
    "        sentences = torch.Tensor()\n",
    "        sent_finished = np.array([False] * batch_size)\n",
    "        print(sent_finished)\n",
    "        for t_i in range(max_sent_len):\n",
    "            # RNN pass\n",
    "            outputs, hidden = self.forward_step(last_tokens, hidden)\n",
    "            decoder_outputs.append(outputs)\n",
    "\n",
    "            # Sample next tokens\n",
    "            if teacher_forcing:\n",
    "                last_tokens = target_embeds[:, t_i].unsqueeze(0)\n",
    "            else:\n",
    "                _, topi = outputs.topk(1)\n",
    "                # topi = topi.squeeze(-1)\n",
    "                last_tokens = self.embed_layer(topi.squeeze(-1))\n",
    "                #print(last_tokens, last_tokens.shape)\n",
    "                print(topi, topi.shape)\n",
    "                sent_finished = sent_finished | topi.squeeze() == 1\n",
    "                print(sent_finished)\n",
    "\n",
    "                input()\n",
    "\n",
    "                # for b_i in range(batch_size):\n",
    "                #     if topi[b_i] == self.word_encoder.EOS_ID:\n",
    "                #         sent_finished[b_i] = True\n",
    "                #     if not sent_finished[b_i]:\n",
    "                #         sentences[b_i].append(\n",
    "                #             self.word_encoder.index2token(topi[b_i]))\n",
    "                \n",
    "                if all(sent_finished):\n",
    "                    break\n",
    "                    \n",
    "        decoder_outputs = torch.cat(decoder_outputs, axis=0).transpose(0, 1)\n",
    "\n",
    "        return decoder_outputs, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"Prey\", \"Center\", \"North\", \"South\", \"East\", \"West\"]\n",
    "\n",
    "word_encoder = OneHotEncoder(vocab)\n",
    "\n",
    "decoder = GRUDecoder(8, 4, word_encoder, word_encoder.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False]\n",
      "tensor([[[-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-0.5092,  0.4545,  0.4991, -0.6824],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-0.5092,  0.4545,  0.4991, -0.6824],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-0.5092,  0.4545,  0.4991, -0.6824],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-0.5092,  0.4545,  0.4991, -0.6824]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[6],\n",
      "         [3],\n",
      "         [6],\n",
      "         [3],\n",
      "         [6],\n",
      "         [3],\n",
      "         [7],\n",
      "         [6],\n",
      "         [6],\n",
      "         [3]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [6],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[6],\n",
      "         [7],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [7],\n",
      "         [6]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [6],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [6],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[6],\n",
      "         [7],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [7],\n",
      "         [6]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [6],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [6],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[6],\n",
      "         [7],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [6],\n",
      "         [7],\n",
      "         [6]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.1207,  0.4461,  0.1331, -0.3018],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [6],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [6],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n",
      "tensor([[[-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439],\n",
      "         [-1.2223,  0.8321, -0.3479,  0.2439]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 10, 4])\n",
      "tensor([[[7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7],\n",
      "         [7]]]) torch.Size([1, 10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.0143, -2.4315, -2.3772,  ..., -2.2706, -1.7719, -1.9388],\n",
       "          [-1.9641, -2.5612, -2.4553,  ..., -2.2944, -1.8945, -1.6837],\n",
       "          [-1.9899, -2.6548, -2.5493,  ..., -2.1778, -1.8877, -1.7256],\n",
       "          ...,\n",
       "          [-2.0450, -2.7311, -2.5879,  ..., -2.0423, -1.8256, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965],\n",
       "          [-2.0175, -2.7131, -2.5777,  ..., -2.0928, -1.8540, -1.7688]],\n",
       " \n",
       "         [[-1.8724, -2.5057, -2.5699,  ..., -2.4312, -1.8118, -2.0060],\n",
       "          [-1.9098, -2.4040, -2.4410,  ..., -2.3952, -1.9116, -1.6924],\n",
       "          [-1.9578, -2.5444, -2.5456,  ..., -2.2343, -1.9084, -1.7252],\n",
       "          ...,\n",
       "          [-2.0398, -2.7287, -2.5890,  ..., -2.0496, -1.8312, -1.8179],\n",
       "          [-2.0450, -2.7310, -2.5879,  ..., -2.0424, -1.8257, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965]],\n",
       " \n",
       "         [[-2.0023, -2.5269, -2.3458,  ..., -2.3258, -1.6529, -2.0698],\n",
       "          [-1.9662, -2.5731, -2.4142,  ..., -2.3138, -1.8133, -1.7721],\n",
       "          [-1.9978, -2.6460, -2.5200,  ..., -2.1824, -1.8397, -1.7820],\n",
       "          ...,\n",
       "          [-2.0450, -2.7310, -2.5879,  ..., -2.0424, -1.8256, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965],\n",
       "          [-2.0175, -2.7131, -2.5777,  ..., -2.0928, -1.8540, -1.7688]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.0431, -2.5411, -2.4218,  ..., -2.1665, -1.6818, -2.0800],\n",
       "          [-1.9842, -2.5995, -2.4548,  ..., -2.2371, -1.8347, -1.7635],\n",
       "          [-2.0066, -2.6706, -2.5400,  ..., -2.1445, -1.8495, -1.7758],\n",
       "          ...,\n",
       "          [-2.0450, -2.7311, -2.5879,  ..., -2.0423, -1.8256, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965],\n",
       "          [-2.0175, -2.7131, -2.5777,  ..., -2.0928, -1.8540, -1.7688]],\n",
       " \n",
       "         [[-2.0180, -2.5489, -2.4182,  ..., -2.2956, -1.6393, -2.1737],\n",
       "          [-1.9759, -2.5607, -2.4248,  ..., -2.3023, -1.8014, -1.8119],\n",
       "          [-1.9594, -2.5858, -2.4722,  ..., -2.2984, -1.8891, -1.6836],\n",
       "          ...,\n",
       "          [-2.0399, -2.7287, -2.5890,  ..., -2.0496, -1.8312, -1.8179],\n",
       "          [-2.0450, -2.7310, -2.5879,  ..., -2.0424, -1.8256, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965]],\n",
       " \n",
       "         [[-1.9046, -2.7212, -2.5866,  ..., -2.3076, -1.8192, -1.8648],\n",
       "          [-1.9349, -2.5523, -2.4647,  ..., -2.2963, -1.8867, -1.6528],\n",
       "          [-1.9789, -2.6411, -2.5574,  ..., -2.1681, -1.8793, -1.7251],\n",
       "          ...,\n",
       "          [-2.0450, -2.7310, -2.5879,  ..., -2.0423, -1.8256, -1.8274],\n",
       "          [-1.9978, -2.6879, -2.5317,  ..., -2.1645, -1.8806, -1.6965],\n",
       "          [-2.0175, -2.7131, -2.5777,  ..., -2.0928, -1.8540, -1.7688]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.rand((10, 8))\n",
    "\n",
    "decoder(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.array(()), np.ones(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
